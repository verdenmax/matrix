# 概率检索模型

## 基本概率统计知识

概率是统计的理论基础，统计是概率的实际应用。

随机试验：可在相同条件下重复进行，试验可能结果不止一个，但能确定所有的可能结果。

随机事件：随机试验中可能出现或可能不出现的情况叫“随机事件”。

条件概率：已知事件A发生的条件下，事件B发生的概率称为A条件下B的条件概率 $P(B|A)$。

乘法公式：$P(AB) = p(A)p(B|A)$

全概率公式：其中$A_1,A_2,...,A_n$ 是整个样本空间的划分 $p(B) = \sum_i p(A_i)p(B|A_i)$

贝叶斯公式：$p(A_j | B) = \frac{p(A_j B)}{p(B)} = \frac{p(B| A_j) p(A_j)}{\sum_i P(B|A_i)p(A_i)}$

两个事件独立：$p(AB) = p(A) p(B)$

随机变量：随机实验的各种可能的结果都能用一个变量的取值范围来表示，则称这个变量为随机变量。

## 概率检索模型

检索系统对用户查询的理解是非确定的，对返回结果的猜测也是非确定的。

概率理论为非确定推理提供了坚实的理论基础，概率检索模型可以计算文档和查询相关的可能性。

概率检索模型是通过概率的方法将查询和文档联系起来。

这里可以定义相关度$R=\{0,1\}$和查询Q，文档D，那么可以通过计算 $p(R=1|Q=q,D=d)$ 来度量文档和查询的相关度。

概率排序原理：如果文档按照与查询的相关概率大小返回，那么该返回结果是所有可能获得结果中效果最好的。

## logistic 回归模型

Logistic 回归是一种非线性回归，Logistic 函数 $y= \frac{1}{1 + e ^{-(\alpha + \beta x)}} = \frac{e^{\alpha + \beta x}}{1 + e^{\alpha + \beta x}}$

logistic 回归可以转化为线性回归，$\frac{y}{1-y}=e^{\alpha + \beta x},ln \frac{y}{1-y} = \alpha + \beta x$

基本思想，认为 $p(R=1|Q,D)$ 是多个特征函数 $f_i(Q,D)$ 的组合。$log \frac{p}{1-p} = \beta_0 + \sum_i \beta_i f_i(Q,D)$

可以通过训练集拟合得到相应系数 $\beta_0 {~} \beta_6$，这样对于新文档，带入公式计算即可。

优点：直接引入数学工具，形式简洁

缺点：特征选择非常困难，实验中效果一般，以文档为样本训练模型，无法解决不同查询之间的差异。

## BIM 模型

二值独立概率模型

BIM模型通过贝叶斯公式展开$p(R=1|Q,D)$进行计算，BIM 是一种生成式模型。

那么对于一个Q定义排序函数

$$
log \frac{p(R=1|D)}{p(R=0|D)} = log \frac{\frac{p(D|R=1)P(R=1)}{p(D)}}{\frac{p(D|R=0)P(R=0)}{p(D)}}
$$

去掉与排序无关因素，即与D无关（因为对D进行排序），$log \frac{p(D|R=1)}{p(D|R=0)}$

那么$p(D|R=1)$和$p(D|R=0)$ 分别表示在相关和不相关情况下生成文档D的概率。

可以认为词项满足某个整体分布，然后从该总体分布中抽样，将抽样出词项连在一起，组成文档。

- 多元贝努利分布，不考虑出现位置，只考虑出现不出现

- 多项式分布，考虑词项的多次出现，不考虑词项的不会出现

这两个分布都不考虑词项的出现位置和次序


!!! example "BIM模型公式"

    对于多元贝努利来说，只考虑是否出现。那么下面进行推导。

    $p_i = p(t_i | R = 1) ,q_i = p(t_i | R = 0)$ 这里前面是指相关情况下，词项出现在文档中的概率(不管文档到底是什么，那么这个概率可能是1)，。

    $log \frac{P(D|R=1)}{P(D|R=0)}$ 约等于 $\sum log \frac{p_i/ ( 1-p_i)}{q_i / (1-q_i)}$


整个文档集合是否和查询相关，分成四个集合。

$p_i$ 就是相关集合中包含t个数除以相关文档总数。
$q_i$ 就是不相关集合中包含t个数除以不相关文档总数。

这里计算 $p_i,q_i$ 参数，对于每个查询，无法事先得到相关文档集和不相关文档集，所以必须进行估计。

可以第一次检索之前的估计，也可以根据上次检索的结果进行估计。


RSJ 权重 $w_i^{RSJ} = log \frac{(r_i + 0.5)(N - R - n_i + r_i +0.5)}{(n_i - r_i + 0.5)(R - r_i + 0.5)}$


BIM缺点：需要估计参数，原始的BIM没有考虑TF，文档长度因素。BIM中同样存在词项独立性假设。

BIM实质上是一个idf权重公式，仅考虑了全局信息，缺少局部信息。因此需要和TF权重配合使用。

## BM25模型

BM25 模型计算公式融合了4个考虑因素：IDF因子，文档长度因子，文档词频和查询词频。并且对三个自由调节因子$(k_1,k_3,b)$ 进行权值的调整。

二重泊松分布，分布形式随参数的取值变化。

$f(k;\lambda)=pr(x=k)= \frac{\lambda^k e^{-\lambda}}{k!}$

在高质量精英文档集中：均值较高，接近正态分布。在整个语料中：均值较低，接近指数分布。

考虑tf，则RSJ 公式则变为 $w = log \frac{p(1-q)}{q(1-p)} = log \frac{p_{tf} q_0}{q_{tf} p_0}$

对于idf因子，假设模型中相关文档数量为0，则退化为 $log \frac{N - df + 0.5}{df + 0.5}$。df是出现词项的文档数目。

查询权重：$\frac{qtf}{k_3 + qtf}$,qtf  是词项在查询中的词频(通常是1)

权重函数：$w = \frac{qtf}{k_3 + qtf} \frac{tf}{k_1 + tf} w^{(1)}$，前面其实代表查询中词项频率，后面是文档中词项频率。

TF 因素: 对于上面单纯的 $\frac{tf}{k_1 + tf}$，加入文档长度因素 $\frac{k_1 tf}{k_1(1 - b + b \frac{l_d}{avg_l}) + tf}$。$l_d$ 是文档长度，$avg_l$ 是平均文档长度。

那么最终 $w(t,d)$ 是上面查询权重函数、TF权重、IDF 权重 的乘积。

基于二重泊松假设，适用于绝大多数语料上IR检索英语。
