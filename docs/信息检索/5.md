# 概率检索模型

## 基本概率统计知识

概率是统计的理论基础，统计是概率的实际应用。

随机试验：可在相同条件下重复进行，试验可能结果不止一个，但能确定所有的可能结果。

随机事件：随机试验中可能出现或可能不出现的情况叫“随机事件”。

条件概率：已知事件A发生的条件下，事件B发生的概率称为A条件下B的条件概率 $P(B|A)$。

乘法公式：$P(AB) = p(A)p(B|A)$

全概率公式：其中$A_1,A_2,...,A_n$ 是整个样本空间的划分 $p(B) = \sum_i p(A_i)p(B|A_i)$

贝叶斯公式：$p(A_j | B) = \frac{p(A_j B)}{p(B)} = \frac{p(B| A_j) p(A_j)}{\sum_i P(B|A_i)p(A_i)}$

两个事件独立：$p(AB) = p(A) p(B)$

随机变量：随机实验的各种可能的结果都能用一个变量的取值范围来表示，则称这个变量为随机变量。

## 概率检索模型

检索系统对用户查询的理解是非确定的，对返回结果的猜测也是非确定的。

概率理论为非确定推理提供了坚实的理论基础，概率检索模型可以计算文档和查询相关的可能性。

概率检索模型是通过概率的方法将查询和文档联系起来。

这里可以定义相关度$R=\{0,1\}$和查询Q，文档D，那么可以通过计算 $p(R=1|Q=q,D=d)$ 来度量文档和查询的相关度。

概率排序原理：如果文档按照与查询的相关概率大小返回，那么该返回结果是所有可能获得结果中效果最好的。

## logistic 回归模型

Logistic 回归是一种非线性回归，Logistic 函数 $y= \frac{1}{1 + e ^{-(\alpha + \beta x)}} = \frac{e^{\alpha + \beta x}}{1 + e^{\alpha + \beta x}}$

logistic 回归可以转化为线性回归，$\frac{y}{1-y}=e^{\alpha + \beta x},ln \frac{y}{1-y} = \alpha + \beta x$

基本思想，认为 $p(R=1|Q,D)$ 是多个特征函数 $f_i(Q,D)$ 的组合。$log \frac{p}{1-p} = \beta_0 + \sum_i \beta_i f_i(Q,D)$

可以通过训练集拟合得到相应系数 $\beta_0 {~} \beta_6$，这样对于新文档，带入公式计算即可。

优点：直接引入数学工具，形式简洁

缺点：特征选择非常困难，实验中效果一般，以文档为样本训练模型，无法解决不同查询之间的差异。

## BIM 模型

二值独立概率模型

BIM模型通过贝叶斯公式展开$p(R=1|Q,D)$进行计算，BIM 是一种生成式模型。

那么对于一个Q定义排序函数

$$
log \frac{p(R=1|D)}{p(R=0|D)} = log \frac{\frac{p(D|R=1)P(R=1)}{p(D)}}{\frac{p(D|R=0)P(R=0)}{p(D)}}
$$

去掉与排序无关因素，即与D无关（因为对D进行排序），$log \frac{p(D|R=1)}{p(D|R=0)}$

那么$p(D|R=1)$和$p(D|R=0)$ 分别表示在相关和不相关情况下生成文档D的概率。

可以认为词项满足某个整体分布，然后从该总体分布中抽样，将抽样出词项连在一起，组成文档。

- 多元贝努利分布，不考虑出现位置，只考虑出现不出现

- 多项式分布，考虑词项的多次出现，不考虑词项的不会出现

这两个分布都不考虑词项的出现位置和次序


整个文档集合是否和查询相关，分成四个集合。

这里计算 $p_i,q_i$ 参数，对于每个查询，无法事先得到相关文档集和不相关文档集，所以必须进行估计。

可以第一次检索之前的估计，也可以根据上次检索的结果进行估计。


BIM缺点：需要估计参数，原始的BIM没有考虑TF，文档长度因素。BIM中同样存在词项独立性假设。

BIM实质上是一个idf权重公式，仅考虑了全局信息，缺少局部信息。因此需要和TF权重配合使用。

## BM25模型

二重泊松分布，分布形式随参数的取值变化。

$f(k;\lambda)=pr(x=k)= \frac{\lambda^k e^{-\lambda}}{k!}$

在高质量精英文档集中：均值较高，接近正态分布。在整个语料中：均值较低，接近指数分布。



