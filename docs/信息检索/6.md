# 基于统计语言建模的IR模型

## 统计语言模型

statistical language modeling

SLM 广泛使用于语音识别和统计机器翻译领域，利用概率统计理论研究语言。

即对于一个文档片段 $d=w_1w_2...w_n$，根据贝叶斯公式$p(w_1w_2...w_n) =p(w_1) \prod p(w_i | w_1 ... w_{i-1})$

这里历史 $w_1...w_{i-1}$，如果无历史，就是一元模型，最近一个历史，就是二元模型，最近N-1个历史：N元模型。

!!! note "数据稀疏性"

    因为数据规模总是有限的，即用于训练模型参数的语料存在稀疏性，如二元模型中恰巧没有出现“国科”组合。

    数据稀疏性导致零概率事件。就会训练出概率为0，但是在训练集上不出现的事件并不代表在新的语料上出现。

    SLM 的一个重要工作就是进行平滑：重新分配概率，即使没有出现的事件也会赋予一个概率。


我们可以将一个有穷状态自动机看作一个确定性语言模型。如果自动机是带概率的，则是概率语言模型。
probabilistic LM 也称统计语言模型SLM。

## 基于SLM 的IR模型

对于单状态概率有穷状态自动机（一元语言模型），其实就是概率分给各个词，Stop状态一个概率。


!!! example "有N个作者，每个人都有对应的文章，如果有一篇新文章Q，查询最有可能是那个作者写的"

    文档模型实际上是某种分布。

    文档和查询都是该总体分布下的一个抽样样本实例。

    可以根据文档估计文档模型，然后求出总体分布。

    计算出该总体分布下抽样出查询的概率。

查询似然模型（Query Likelihood Model）

$P(D|Q) = \frac{P(Q|D)P(D)}{P(Q)}$

$P(D)$ 是与查询无关的量。所以计算 $P(Q|D)$ 记在D模型下，生成Q查询的概率。

$RSV(Q,D) = P(Q|D) = P(Q|M_D) = P(q_1 q_2 ... q_m | M_D) = p(q_1 |M_D) p(q_2 | M_D) ... p(q_m|M_D)$

现在检索问题就转化为估计文档D的一元语言模型，即求所有词项概率 $p(w|M_D)$

D与Q都是实例，实例是不会产生实例的。所以不能理解为 $P(w|M_d)$ 等于  $P(w|D)$。

!!! note "$M_d$ 的估计"

    最大似然估计 MLE

    记 $\theta = (p(w_1 | M_D) , p(w_2 | M_D),.....,p(w_L | M_D))$

    则其实就是估计 $argmax P(D | \theta)$

    使用文本生成多项式模型，记录每个词项发生次数

    $f = n! \prod \frac{\theta_i^{x_i}}{x_i!}$，其中$x_i$ 就是第i个词项在文档中出现次数。

    条件极值，使用拉格朗日乘数法求解。

    $\theta = \frac{x_i}{|D|}$

    因为数据稀疏性，所以需要平滑。

数据平滑的一般形式

$$
p(w|D)
=
\begin{cases}
P_{DML} (w|D), w \in D,(折扣后的MLE) \\
\alpha_D p(w|REF),otherwise
\end{cases}
$$

其中取

$$
\alpha_d = \frac{1 - \sum_{w\in D} P_{DML}(w |D)}{\sum_{w \notin D} p(w | REF)},
p(w|REF) = p(w|C) = \frac{\sum_D c(w,D)}{\sum_w \sum_D c(w,D)}
$$

这里的C是集合语言模型，就是w出现次数除以所有次出现的次数。


最终排名函数即为

$RSV(Q,D) = \prod_{w \in Q} p(w|D)^{c(w,Q)} \Rightarrow \sum_{w\in Q \cap D} c(w,Q) log \frac{P_{DML}(w|D)}{\alpha_D p(w|C)} + |Q| log \alpha_D$

!!! example "翻译模型"

    相比与上面的基本QLM模型。

    基本QLM模型不能解决词语失配问题，例如电脑 vs 计算机是同一个次。

    所以假设通过一个有噪声香农信号 

    $P(Q|D) = \prod_i P(q_i |D) = \prod_i \sum_j P(q_i |w_j) p(w_j | M_D)$，其中 $p(q_i | w_j)$ 是翻译概率

还有KL矩阵模型

## SLMIR模型讨论


SLMIR vs VSM

其中词项频率直接在模型中使用。但是在SLM中没有放缩变化。

本质上概率表示进行了长度归一化，VSM中余弦归一化也做了类似工作。

文档中词项频率和文档集频率混合后和idf效果相当。


假设：两个模型都假设词项之间是独立的。

SLMIR 比向量空间中假设表述更清楚，因此比VSM具有更好的理论基础。
