# 信息检索

## 介绍

给定需求，从信息库中找出与之最匹配的信息。

数据形式：无固定结构的自由文本；结构化数据。

就是寻找匹配的学科，即定义并计算某种匹配**相似度**的学科。


## 布尔检索

本课程主要关注文本检索。


文档集 Collection：由固定数目的文档组成
目标：返回与用户需求相关的文档并辅助用户来完成某项任务


!!! note "评价指标"
    
    正确率 Precesion: 返回结果文档中正确的比例
    
    召回率 Recall：全部相关文档中被返回的比例


**布尔检索是指利用AND、OR或者NOT操作符将词项连接起来的查询。**


其中**暴力寻找**实现简单，很容易支持文档动态变化；但是速度慢，处理NOT必须到末尾才能判断，不支持其他操作，不支持检索结果排序（只返回较好的结果）。

!!! note "词项文档关联矩阵"

    每一行代表一个词项

    每一列代表一个文档

    一个位置只有0、1

    若这个文档包含该单词，则该位置上为1，否则为0。


有了词项文档关联矩阵，每一行一列都可以看作一个向量。
那么布尔查询就可以转化为这些向量的位运算。

**当词汇表的大小（词项个数）很大，该矩阵将非常大，但是高度稀疏Sparse**。

!!! note "倒排索引"

    对于每个词项t，记录所有包含t的文档列表，即每个词项t一个变长数组。

    每个词项对应的文档列表要安装 Docid 排序。

倒排索引构建

!!! note "文本预处理"

    - 词条化 Tokenization：将字符序列切分为词条
    - 规范化 Normalization：将文档和查询中的词项映射到相同的形式 U.S.A. 到 USA
    - 词干还原 Stemming：将同一词汇不同形式还原到词根
    - 停用词去除：去除高频词 the，a


!!! note "词条序列"

    词条序列就是一个二元组：\<词条，docID\>


按照词项排序，然后每个词项按照docID排序。

拆分成词项和倒排记录表。

词项记录这个词、出现次数和指向该词倒排记录表的指针。

!!! note "倒排记录表-布尔查询"

    对于AND，就是对两个倒排记录表进行合并，线性时间复杂度,**所以倒排记录表需要docID排序**

    OR和NOT同上


查询优化：每次从最小的开始合并可以尽量提前结束合并。


**布尔查询构建复杂，构建不当检索结果过多或者过少。并且没有充分利用词项的频率信息、不能对检索结果进行排序。**


## 构建索引

对于普通的索引构建时，对于每一个词项来说，其倒排记录表不到最后一篇文档都是不完整的。
而当处理大规模语料时，不能将前面产生的倒排记录表全部放在内存中，然后进行排序。

!!! note "BSBI算法 Blocked sort-based Indexing"


    分块，每一块的倒排记录排序，然后将不同分块合并成一个大的倒排索引。

    需要维护一个全局词典：词项到整形词项ID的映射表

    待合并的倒排记录表，只包含整形词项ID

    可以多项进行一起归并。


!!! note "SPIMI Single pass in-memory indexing"

    每个块都产生一个独立的词典，不需要在块间进行 term -termID 映射

    倒排记录表不需要排序，词典进行排序即可（词典有指向倒排记录表的指针）

    合并时对索引进行合并即可

!!! note "动态索引"

    主索引+辅助索引，磁盘上维护一个大的主索引，新文档放入小的辅助索引中，同时搜索两个索引，
    然后合并结果。定期合并。可以进行对数合并


## 索引压缩

压缩，节省开销、加快速度

一般要求 无损压缩、随机访问。


词汇表大小会随着文档集的增长而增长

经验规律：Heaps 定律 $M=kT^b$

M 是词汇表大小，T是文档集大小（所有词条个数），参数k，b经典取值为 $30 \le k \le 100$ 以及 b约等于 0.5。

Heaps 定律告诉我们随着文档集规模的增长词项的增长情况。

Zipf 定律，第 i 常见的词项的频率 $cf_i$ 和 $\frac{1}{i}$ 成正比。

词项频率：词项ti在所有文档中出现的次数

### 压缩词项

将所有的词典看作一个单一字符串，然后使用词项指针指向该词项。

可以减少指针数量，但是需要引入词项长度计数。

还可以使用前端编码，对于公共前缀来说，

### 倒排记录表压缩

关键是对每条倒排记录进行压缩。

思想1:存储docID间隔，而不是docID，，因为间隔更小

可变字节VB码，设定一个专用为（高位）c作为延续位，若间隔小于7位，则c置为1，间隔放入后7未。
否则将间隔高7位放入当前字节，c置为0，剩下的位同样方法处理。

基于字节编码，从高到底编码。


$\gamma$ 编码，基于位的编码

先介绍一元码，将n表示为 n个1后接一个0。

$\gamma$ 编码，将一个数分为长度和偏移两部分。
偏移就是这个数的二进制表示，去掉最高位的1。

然后长度就是这个偏移的二进制长度，使用一元码表示。

将长度部分和偏移部分联接起来得到gamma编码。

$\gamma$ 编码的长度均是奇数，在最优编码长度的2倍左右（不依赖间隔的分布，是通用编码）。

前缀无关，保证了解码的唯一性。是无参数编码，不需要通过拟合得到参数，


组变长整数编码

用前面的一些位来概括剩下位的存储格式。


## 通配查询与拼写矫正



对每个词项t，保存所有包含t的文档列表。词典的两个基本功能：1、存储词项字符串 2、定位词项

词典是指存储词项词汇表的数据结构。

词项词汇表指的是具体数据

对于词项，需要存储文档频率 DF，指向倒排记录表的指针。


用于词项定位的数据结构：哈希表和树。

!!! note "Hash table"

    - 每个词项通过哈希函数映射成一个整数。
    - 尽可能避免冲突。
    - 在哈希表中定位速度块，优于树，查询时间是常数。
    - 无法处理词项的微小变形，不支持前缀搜索。
    - 如果词汇表不断增大，需要定期对所有词项进行重新哈希

!!! note "tree"

    - 树支持前缀查询
    - 搜索速度 log 级别
    - 使得二叉树重新保持平衡开销很大

### 通配查询

mon*,以mon开头。

如果使用B-树很简单，直接返回区间 $mon \le t < moo$

*mon 以mon结尾

将所有词项倒转，然后建立一棵树，这样就相当于找区间 $nom \le t < non$


词项中间的 * 进行处理，可以按照上面的方法中，分布查找然后求交集。这种做法时间复杂度很大。

轮排 permuterm 算法，将每个通配查询旋转，使得 * 出现在末尾。

对于一个词项 hello$,其中  $ 代表结尾。轮转加入B-树中，

轮排索引查询时，将查询进行旋转，将通配符旋转到右边。同以往一样查找B-树，但是空间占用比较大。

K-gram 索引：枚举一个词项中所有连续 k 个字符构成 k-gram。

2-gram 称为二元组，3-gram称为三元组。同样使用 $ 作为特殊字符。

相当于对词项再构建一个倒排索引（二级索引）。

- 词典-文档的倒排索引基于词项返回文档
- k-gram索引用于查询词项，即基于查询所包含k-gram来查找所有词项。

!!! note "利用2-gram 进行查询"

    如查询 mon*

    进行布尔查询 $m AND mo AND on

    这个布尔查询返回所有以前缀mon开头词项（可能会返回伪正例，所以要做好过滤）

    剩下的词项在词项-文档倒排索引中查找文档

K-gram 索引的空间消耗小，但是需要进行后过滤。轮排索引不需要进行后过滤。

## 拼写矫正

### 拼写错误检测

非词汇（错误的词现实不存在）的纠正一般来说不需要考虑上下文。

!!! note "非词汇纠正"

    词典中不存在的词均视为错误

    一般来说词典越大越好（web词典很大，但是充满拼写错误）

    产生候选：与错误书写的单词相似的真实词汇

    选择最好的编辑词：1.最短加权编辑距离2.最高噪声通道概率

真实词汇的纠正通常需要考虑上下文。


!!! note "真实词汇"
    
    对于w产生候选集：发音相似的候选词、拼写相似的候选词、w本身。

    选择最佳候选词，将拼写错误视为经过噪声通道的输出上下文敏感 ，需要考虑周围的文字是否合适。


1. 纠正待索引文档
2. 纠正用户的查询

词独立法，只检查每个单词本身的拼写错误。

上下文敏感法，纠错时要考虑周围的单词。


!!! note "噪声通道模型"

    噪声通道=贝叶斯定理

    一个拼写错误的词x，正确词项w，这时需要找到 $w=argmax p(w|x) = argmax p(x|w)p(w)$

    对于候选词，找到编辑距离小的词。找到最好的代价比较高，一般是选择一个较好的候选集，然后从中选择好的候选词。

    首先找到一个好的候选集，然后从中找到最优的前k个答案。

    然后含有T个词条的大文本语料，c（w) = occurrences of w，$p(w) = \frac{C(w)}{T}$。

    通过编辑概率生成混淆矩阵，得到似然概率

!!! note "上下文敏感"

    其实就是选择w，使得 $w = argmax p(w|x_1x_2..x_n)$ 概率最大，
